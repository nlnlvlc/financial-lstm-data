{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1oYFL-29MUpr13pxnfLJBRIMYP-tQHzbF",
      "authorship_tag": "ABX9TyMe2a3kxTFYB2mAIpAb5BAw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nlnlvlc/financial-lstm-data/blob/main/preprocessing_financial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code prepares the data to run through two sentiment analysis models:\n",
        "\n",
        "*   an **AT-LSTM** (Attention based Long-Short Term Memory model)\n",
        "*   a **Bi-LSTM-AN** (BiDirectional Long-Short Term Memory & Adverserial Neural Network Hybrid model)\n",
        "\n",
        "Before running the data through these models, the data previously cleaned [here](https://colab.research.google.com/drive/1IjLgsdhgxp0GHsvgoPNi5vyd7gLawzwt#scrollTo=OWXNCQeJu68o) most be pre-processed to identify words with significant semantic meaning and their **hypernyms**: sets of words which categorizes or better generalizes another word.\n",
        "\n",
        "*(Ex. \"Color\" or \"Fruit\" are both hypernyms for \"Orange\")*\n",
        "\n",
        "The full project can be found [here](https://github.com/Alex-Gideon/635Group3Project/tree/main)."
      ],
      "metadata": {
        "id": "LMebHsDCw3xb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qDVqMx50xAn1",
        "outputId": "f2b10413-329e-43b2-b49e-3860f0a4045b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import wordnet as wn\n",
        "from os import walk\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "import csv\n",
        "\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import stops words\n",
        "stop_words = set(stopwords.words('english'))"
      ],
      "metadata": {
        "id": "v0MlT4EOxGbI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check if word has associated hypernyms\n",
        "def check_for_hypernim(token):\n",
        "    hypernims = []\n",
        "    for i in range(15):\n",
        "        try:\n",
        "            hypernims1 = []\n",
        "            for i, j in enumerate(wn.synsets(token)):\n",
        "                for l in j.hypernyms():\n",
        "                    hypernims1.append(l.lemma_names()[0])\n",
        "            token = hypernims1[0]\n",
        "            hypernims.append(hypernims1)\n",
        "        except IndexError:\n",
        "            hypernims.append([token])\n",
        "\n",
        "    return hypernims"
      ],
      "metadata": {
        "id": "Z53fdngRxJ_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#applies hypernym\n",
        "def process_text(fileName):\n",
        "  df = pd.read_csv(fileName)\n",
        "\n",
        "  #holds collection of each word and their hypernyms\n",
        "  processed = []\n",
        "  #holds label for each word\n",
        "  labels = []\n",
        "\n",
        "  #processes text in each row by filtering each sentence\n",
        "  #and tagging with appropriate hypernyms\n",
        "  #if no hypernym exists, the base word is applied as its own hypernym\n",
        "  for index, row in df.iterrows():\n",
        "    label = row['label']\n",
        "    line = row['text']\n",
        "    words = line.split()\n",
        "\n",
        "    filtered_sentence = []\n",
        "\n",
        "    x = []\n",
        "    #removes stop words from list of words\n",
        "    for r in words:\n",
        "\n",
        "        if not r in stop_words:\n",
        "            filtered_sentence.append(r)\n",
        "\n",
        "    tagged = nltk.pos_tag(filtered_sentence)\n",
        "    #removes unusable characters and undesired tagged words\n",
        "    for i in tagged:\n",
        "        if len(i[0]) != 0 or len(i[0]) != 1:\n",
        "            if i[1] == 'IN' or i[1] == 'DT' or i[1] == 'CD' or\n",
        "            i[1] == 'CC' or i[1] == 'EX' or i[1] == 'MD' or   i[1] == 'WDT' or\n",
        "            i[1] == 'WP' or i[1] == 'UH' or i[1] == 'TO' or i[1] == 'RP' or\n",
        "            i[1] == 'PDT' or i[1] == 'PRP' or i[1] == 'PRP$' or i[0] == 'co':\n",
        "                # print(i[0])\n",
        "                continue\n",
        "            else:\n",
        "                x.append(i[0].rstrip(\".,?!\"))\n",
        "    #check if a hypernym exists for remaining words\n",
        "    #if no, add word as its own hypernym 15 times\n",
        "    #if yes, apply hypernyms to word\n",
        "    for i in x:\n",
        "        l = []\n",
        "        l.append(i)\n",
        "        hype = check_for_hypernim(i)\n",
        "        if len(hype) == 0:\n",
        "            hype.append(i)  # 1\n",
        "            hype.append(i)  # 2\n",
        "            hype.append(i)  # 3\n",
        "            hype.append(i)  # 4\n",
        "            hype.append(i)  # 5\n",
        "            hype.append(i)  # 6\n",
        "            hype.append(i)  # 7\n",
        "            hype.append(i)  # 8\n",
        "            hype.append(i)  # 9\n",
        "            hype.append(i)  # 10\n",
        "            hype.append(i)  # 11\n",
        "            hype.append(i)  # 12\n",
        "            hype.append(i)  # 13\n",
        "            hype.append(i)  # 14\n",
        "            hype.append(i)  # 15\n",
        "        for hyper in hype:\n",
        "            l.append(hyper[0])\n",
        "        processed.append(l)\n",
        "        labels.append(label)\n",
        "\n",
        "  print(len(processed))\n",
        "  print(len(labels))\n",
        "\n",
        "  #test that word and label order is correct\n",
        "  print(f'Label 0: {labels[0]}')\n",
        "  print(f'Processed 0: {processed[0]}')\n",
        "\n",
        "  #create list of hyponyms\n",
        "  word_only = [x[0] for x in processed]\n",
        "\n",
        "  dict = {\"label\": labels, \"hypernyms\":word_only}\n",
        "  #dataframe holding each word and their hypernyms\n",
        "  processed_df = pd.DataFrame(processed)\n",
        "  #dataframe holding each word and their respective labels\n",
        "  label_df = pd.DataFrame(dict)\n",
        "  label_df.head(5)\n",
        "  #join both dataframes and drop duplicate column\n",
        "  #to ensure label/word order is maintained\n",
        "  joined_df = label_df.join(processed_df)\n",
        "  joined_df.drop(['hypernyms'], axis=1, inplace=True)\n",
        "\n",
        "  #based on file name, save processed and joined dataframes to respective files\n",
        "  if fileName == '/experiment-1/financial/datasets/clean_financialpc.csv':\n",
        "\n",
        "    processed_df.to_csv(\n",
        "        '/experiment-1/financial/datasets/hypernyms_financialpc.csv',\n",
        "        header=False,\n",
        "        index=False\n",
        "        )\n",
        "\n",
        "    joined_df.to_csv(\n",
        "        '/experiment-1/financial/datasets/train_financialpc.csv',\n",
        "        header=False,\n",
        "        index=False)\n",
        "    print(\"clean_financialpc has been processed\")\n",
        "\n",
        "  if fileName == '/experiment-1/financial/datasets/clean_financialfull.csv':\n",
        "    processed_df.to_csv(\n",
        "        '/experiment-1/financial/datasets/hypernyms_financialfull.csv',\n",
        "        header=False,\n",
        "        index=False\n",
        "        )\n",
        "\n",
        "    joined_df.to_csv(\n",
        "        '/experiment-1/financial/datasets/train_financialfull.csv',\n",
        "        header=False,\n",
        "        index=False\n",
        "        )\n",
        "    print(\"clean_financialfull has been processed\")\n"
      ],
      "metadata": {
        "id": "adRCNNoCKgO3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#cleaned datasets to be processed\n",
        "datasets = ['/experiment-1/financial/datasets/clean_financialpc.csv',\n",
        "            '/experiment-1/financial/datasets/clean_financialfull.csv']"
      ],
      "metadata": {
        "id": "havKhN7hM5dt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#process all datasets and verify the correct number of items and the labelling\n",
        "for dataset in datasets:\n",
        "  process_text(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zelZnaVVM_8E",
        "outputId": "56ea36da-83a4-43ac-bf9c-da7426e927a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "82078\n",
            "82078\n",
            "Label 0: 1\n",
            "Processed 0: ['ReutersPayments', 'ReutersPayments', 'ReutersPayments', 'ReutersPayments', 'ReutersPayments', 'ReutersPayments', 'ReutersPayments', 'ReutersPayments', 'ReutersPayments', 'ReutersPayments', 'ReutersPayments', 'ReutersPayments', 'ReutersPayments', 'ReutersPayments', 'ReutersPayments', 'ReutersPayments']\n",
            "clean_financialpc has been processed\n",
            "56663\n",
            "56663\n",
            "Label 0: 7\n",
            "Processed 0: ['According', 'match', 'lighter', 'fuel', 'substance', 'matter', 'concern', 'interest', 'curiosity', 'cognitive_state', 'psychological_state', 'condition', 'state', 'administrative_district', 'district', 'region']\n",
            "clean_financialfull has been processed\n"
          ]
        }
      ]
    }
  ]
}